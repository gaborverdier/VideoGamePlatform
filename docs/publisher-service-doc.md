# Roadmap : Publisher Service (Java)
**Projet JVM-Data 2025**

## üéØ Objectif du Service
Simuler le comportement d'un √©diteur de jeux vid√©o qui :
1.  G√®re son catalogue de jeux (initialis√© via le dataset VGSales).
2.  Publie des mises √† jour (Patchs) et des modifications de m√©tadonn√©es.
3.  Analyse les retours techniques (Crashs) et les agr√©gats de qualit√© (Ratings).

---

## üìÖ Phase 1 : Initialisation & Structure (Jours 1-2)
*L'objectif est d'avoir un projet qui compile et une base de donn√©es locale.*

- [ ] **Cr√©ation du module Java**
    - [cite_start]Cr√©er le dossier `publisher-service-java` dans le monorepo[cite: 185].
    - Initialiser avec Maven ou Gradle (Java 17 ou 21 recommand√©).
    - Ajouter les d√©pendances :
        - `kafka-clients`
        - `kafka-avro-serializer`
        - `spring-boot-starter-data-jpa` (ou Hibernate pur)
        - Driver BDD (PostgreSQL ou MySQL).
- [ ] **Connexion au module commun**
    - [cite_start]S'assurer que le service d√©pend du module `common-avro-schemas` pour r√©cup√©rer les objets Avro g√©n√©r√©s[cite: 199].
- [ ] **Configuration Docker**
    - [cite_start]V√©rifier que le `docker-compose.yml` racine lance bien ta base de donn√©es (ex: `publisher-db`)[cite: 179].
- [ ] **Configuration Application**
    - Configurer `application.properties` (ou `.yml`) :
        - URL connexion BDD.
        - Bootstrap servers Kafka.
        - URL Schema Registry.

## üíæ Phase 2 : Mod√©lisation des Donn√©es (Jours 3-4)
*D√©finir comment les donn√©es sont stock√©es et √©chang√©es.*

- [ ] **D√©finition des Sch√©mas Avro (dans `common-avro-schemas`)**
    - Collaborer avec l'√©quipe pour valider les √©v√©nements que tu vas **produire** :
        - [cite_start]`GamePatchedEvent` (jeu, version, changelog)[cite: 75].
        - [cite_start]`GameMetadataUpdatedEvent`[cite: 76].
    - Collaborer pour valider les √©v√©nements que tu vas **consommer** :
        - [cite_start]`GameCrashReportedEvent`[cite: 78].
        - [cite_start]`GameRatingAggregatedEvent`[cite: 79].
- [ ] **Mod√®le de donn√©es relationnel (JPA/SQL)**
    - [cite_start]Cr√©er les entit√©s Java pour ta BDD locale[cite: 83]:
        - `Game` (id, titre, genre, platform, current_version).
        - `CrashReport` (id, game_id, error_code, timestamp).
        - `PatchHistory` (id, game_id, version, date).
        - (Optionnel) `ReviewStats` (pour stocker les agr√©gats re√ßus).

## üöÄ Phase 3 : Injection des Donn√©es VGSales (Jour 5)
*Le Publisher doit poss√©der des jeux pour pouvoir les patcher.*

- [ ] **Script d'import VGSales**
    - [cite_start]T√©l√©charger le dataset `vgsales.csv`[cite: 326].
    - Cr√©er un service (`VGSalesLoader`) qui lit le CSV au d√©marrage.
    - [cite_start]Mapper les colonnes (Name, Platform, Genre, Publisher) vers ton entit√© `Game`[cite: 329].
    - Sauvegarder ces jeux dans ta BDD locale (`publisher-db`).
    - *Note :* Filtrer pour ne garder que les jeux de l'√©diteur "Nintendo" ou "Activision" par exemple, pour simuler un √©diteur sp√©cifique, ou tout importer si tu es l'√©diteur "Global".

## üì° Phase 4 : Impl√©mentation Kafka Consumers (Jours 6-7)
*√âcouter ce qui se passe sur la plateforme.*

- [ ] **Consumer : Rapports de Crashs**
    - [cite_start]Cr√©er un `KafkaConsumer` pour le topic `game-crash-reported`[cite: 28].
    - Action : √Ä chaque r√©ception, enregistrer le crash en BDD dans la table `crash_reports`.
    - *Logique m√©tier :* Si un jeu d√©passe X crashs, logger une alerte "URGENT PATCH NEEDED".
- [ ] **Consumer : Analytics**
    - [cite_start]Cr√©er un `KafkaConsumer` pour le topic `game-rating-aggregated` (venant du service Kotlin)[cite: 79].
    - Action : Mettre √† jour les stats du jeu en BDD ou logger la tendance ("Le jeu X a une moyenne de 4.5/5").

## üì¢ Phase 5 : Impl√©mentation Kafka Producers (Jours 8-9)
*Agir sur le monde.*

- [ ] **Producer : Publication de Patch**
    - Cr√©er un service `PatchService`.
    - M√©thode `deployPatch(String gameId, String version, String content)`.
    - Action 1 : Mettre √† jour la version du jeu en BDD et ajouter une entr√©e dans `PatchHistory`.
    - [cite_start]Action 2 : Produire un √©v√©nement Avro `GamePatchedEvent` dans le topic `game-patched`[cite: 21].
- [ ] **Producer : Mise √† jour M√©tadonn√©es**
    - M√©thode `updateGameDetails(...)`.
    - [cite_start]Produire un √©v√©nement sur `game-metadata-updated`[cite: 22].

## üéÆ Phase 6 : API & Simulation (Jours 10-11)
*Rendre le service utilisable.*

- [ ] **Interface de Contr√¥le (REST API simple)**
    - Exposer des endpoints pour d√©clencher tes actions :
        - `POST /api/games/{id}/patch` : D√©clenche l'envoi d'un patch.
        - `GET /api/reports/crashes` : Affiche les crashs re√ßus.
- [ ] **G√©n√©rateur Automatique (Optionnel mais recommand√©)**
    - Cr√©er un `ScheduledTask` qui, toutes les X minutes, s√©lectionne un jeu au hasard dans la BDD et publie un patch mineur (pour g√©n√©rer du trafic Kafka automatiquement pendant la d√©mo).

## üìù Phase 7 : Documentation & Tests (Jours 12-13)
- [ ] **Tests Unitaires** : Tester la s√©rialisation Avro et la logique JPA.
- [ ] [cite_start]**Rapport** : R√©diger la partie "Architecture Publisher" pour le rendu final[cite: 355].
- [ ] **Sch√©ma** : G√©n√©rer un sch√©ma de ta BDD et de tes flux Kafka.

---

## üõ† R√©sum√© Technique
* **Langage** : Java
* **Type d'app** : Spring Boot (recommand√©) ou Java pur.
* **BDD** : PostgreSQL (via Docker).
* **Kafka Topics (Input)** : `game-crash-reported`, `game-rating-aggregated`, `game-crash-stats`.
* **Kafka Topics (Output)** : `game-patched`, `game-metadata-updated`.
* **Source de donn√©es** : Fichier `vgsales.csv`.